{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO: \n",
    "1. Create parameters for specifying units for a PV\n",
    "2. Create parameters for specifying rounding for an axis"
   ],
   "id": "1554eb895c7e1215"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Process: \n",
    "1. Create DataFrames for all PVs of interest over a 6-month timeframe: `df_pv = pd.read_csv()`\n",
    "2. Remove charges less than a given value, like 15 pC: `df_charge = df_charge[df_charge[pv_charge] >= cutoff]`\n",
    "3. Merge all DataFrames together, by Timestamp, given a margin of error:\n",
    "```\n",
    "df_charge_ycor = merge_with_margin_on_timestamp(df_charge, df_ycor, time_margin_seconds=10)\n",
    "df_correl = merge_with_margin_on_timestamp(df_charge_ycor, df_homc1, time_margin_seconds=10)\n",
    "```\n",
    "4. Plot the correlation with error due to the spread of certain values:\n",
    "```\n",
    "plot_correlation(df_correl, pv_y=\"SCOP:AMRF:RF01:AI_MEAS1\", pv_x=\"YCOR:GUNB:293:BACT\", pv_charge=\"TORO:GUNB:360:CHRG\", charge_val=50.0, charge_tolerance=0.5, plot_error_bars=True, low_vary_column=\"YCOR:GUNB:293:BACT\", high_vary_column: \"SCOP:AMRF:RF01:AI_MEAS1\", error_tolerance=0.000015, y_vary=True)\n",
    "```\n",
    "\n",
    "Note: \n",
    "- To determine which `charge_val` to choose, one must run the method `separate_df_by_charges` from the `ChargeSeparator` class. \n",
    "- Trial and error must be done to determine what values to use for parameters `charge_tolerance` and `error_tolerance`. "
   ],
   "id": "3fe26babf30aa389"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "7a51b820a77c96e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:12:55.878950Z",
     "start_time": "2024-08-01T00:12:55.876487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import charge_separator as cs\n",
    "import sys\n",
    "sys.path.append('/Users/jonathontordilla/Desktop/hombom24/archive-analysis/lcls-tools-plots/archiver_plotter')\n",
    "import archiver_plotter as ap # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "9dbd128ca0ad2562",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Goal: remove charges less than a given value for a charge DataFrame",
   "id": "d2c4f4d5e9d6ee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:12:55.882044Z",
     "start_time": "2024-08-01T00:12:55.880118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_charges_below_value(df: pd.DataFrame, pv_charge: str, min_charge_value: float) -> pd.DataFrame:\n",
    "    return df[df[pv_charge] >= min_charge_value]"
   ],
   "id": "55afa3c5c3826d51",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Goal: merge all DataFrames together, by Timestamp, given a margin of error",
   "id": "13b433a0c6d9ad6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:12:55.885581Z",
     "start_time": "2024-08-01T00:12:55.882698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_with_margin_on_timestamp(df_1: pd.DataFrame, df_2: pd.DataFrame, time_margin_seconds: float): \n",
    "    \"\"\"Merges two DataFrames on similar timestamps, where timestamps differ by less than the time specified by the time_margin parameter.\n",
    "    \n",
    "    Creates additional columns that store the time difference between the true and comparison timestamps. \n",
    "    \n",
    "    :param df_1: First DataFrame with a Timestamp column.\n",
    "    :param df_2: Second DataFrame with a Timestamp column.\n",
    "    :param time_margin_seconds: The time margin between two timestamps as given in seconds, useful for defining the propagated error for a correlation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Must convert the values in the Timestamp column to datetime objects\n",
    "    df_1[\"Timestamp\"] = pd.to_datetime(df_1[\"Timestamp\"])\n",
    "    df_2[\"Timestamp\"] = pd.to_datetime(df_2[\"Timestamp\"])\n",
    "    \n",
    "    \"\"\"Use the pandas method merge_asof to merge the DataFrames within a tolerance value (pandas.pydata.org/docs/reference/api/pandas.merge_asof.html). \n",
    "    According to the pd.merge_asof() function, the first DataFrame parameter in the function defines what the second DataFrame is compared to. \n",
    "    Therefore, the first DataFrame will have a time-axis uncertainty of 0. \n",
    "    The second DataFrame will have some uncertainty ranging from 0 to the time_margin_seconds value. \n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the time difference between the second and first DataFrames and add a new column to the second DataFrame\n",
    "    df_merged = pd.merge_asof(df_1, df_2, on=\"Timestamp\", direction=\"nearest\", tolerance=pd.Timedelta(f\"{time_margin_seconds}s\"))\n",
    "    df_merged[f\"{df_2.columns[1]} Time Uncert\"] = df_merged[df_2.columns[1]] - df_merged[df_1.columns[1]]  # get time uncertainty\n",
    "    \n",
    "    # Convert values in the Timestamp column back to String objects, remove NaN rows, and return \n",
    "    timestamp_list = df_merged[\"Timestamp\"].to_list()\n",
    "    df_merged[\"Timestamp\"] = timestamp_list\n",
    "    return df_merged.dropna(how=\"any\")"
   ],
   "id": "1f978aec5a652295",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Goal: plot the correlation with a line of best fit, either with or without error bars",
   "id": "84d6fbf93df375ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:12:55.889468Z",
     "start_time": "2024-08-01T00:12:55.886753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_clusters(df: pd.DataFrame, low_vary_column: str, error_tolerance: float) -> list[pd.DataFrame]: \n",
    "    \"\"\"Create a list of DataFrames, each with data points with similar x values but varying y values.\n",
    "    \n",
    "    :param df: DataFrame with at least and x_column and a y_column from which to create clusters. \n",
    "    :param low_vary_column: String of the title of the column with points by which to create clusters.\n",
    "    :param error_tolerance: The tolerance range of the x values in whatever units the x_column is. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.sort_values(by=low_vary_column).reset_index(drop=True)\n",
    "    \n",
    "    # group the values by similar x values\n",
    "    df_groups: list[pd.DataFrame] = []\n",
    "    curr_group: list[pd.Series] = []\n",
    "    \n",
    "    for i, curr_row in df.iterrows(): \n",
    "        if not curr_group: \n",
    "            curr_group.append(curr_row)  # add current row to the group if no rows added yet\n",
    "        else: \n",
    "            # if the row is not empty, check if the item in the cell in the x_column of the current row in within range\n",
    "            if abs(curr_group[-1][low_vary_column] - curr_row[low_vary_column]) <= error_tolerance: \n",
    "                curr_group.append(curr_row)\n",
    "            else: \n",
    "                # if it is out of range, add the current group as a DataFrame to the list\n",
    "                df_groups.append(pd.DataFrame(curr_group))\n",
    "                curr_group = [curr_row]  # create a new group with the new row\n",
    "    \n",
    "    # handle remaining rows and add to a DataFrame and then to the DataFrame list\n",
    "    if curr_group: \n",
    "        df_groups.append(pd.DataFrame(curr_group))\n",
    "        \n",
    "    return df_groups"
   ],
   "id": "4d4f50c1676492e7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:12:55.902690Z",
     "start_time": "2024-08-01T00:12:55.900560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_means_and_stds(df_groups: list[pd.DataFrame], high_vary_column: str) -> list[tuple[float, float]]:\n",
    "    \"\"\"Create a 2D list with means and standard deviations for each cluster.\n",
    "    \n",
    "    Returns a list of pairs (mean of each cluster, std for each cluster). \n",
    "    \n",
    "    :param df_groups: List of DataFrames, each one representing a different cluster. \n",
    "    :param high_vary_column: String of the title of the column with the varying points in a cluster. \n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for group_index in range(len(df_groups)):\n",
    "        mean = float(np.mean(df_groups[group_index][high_vary_column]))\n",
    "        curr_std = float(np.std(df_groups[group_index][high_vary_column]))\n",
    "        result.append((mean, curr_std))\n",
    "    return result"
   ],
   "id": "2dce288f94ff290a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:12:55.922362Z",
     "start_time": "2024-08-01T00:12:55.917876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_correlation(df: pd.DataFrame, pv_y: str, pv_x: str, pv_charge: str, charge_val: float, \n",
    "                     charge_tolerance: float, plot_error_bars: bool, low_vary_column: str, \n",
    "                     high_vary_column: str, error_tolerance: float, y_vary=True): \n",
    "    \"\"\"Plots the correlation between two columns in a DataFrame, separated by a specific charge value (pC). \n",
    "    \n",
    "    The DataFrame will be modified to only include rows that contain a charge value within a given tolerance percentage range (0.0-1.0). \n",
    "    \n",
    "    :param df: DataFrame with a Timestamp column and an arbitrary amount of PV columns.\n",
    "    :param pv_y: String representation of the PV to be plotted on the y-axis.\n",
    "    :param pv_x: String representation of the PV to be plotted on the x-axis.\n",
    "    :param pv_charge: String representation of the charge PV by which to filter the DataFrame.\n",
    "    :param charge_val: The charge value in pC by which to separate out rows. \n",
    "    :param charge_tolerance: The percentage tolerance between 0.0-1.0 (inclusive) to inform how large of a spread of charge values are kept in the DataFrame. \n",
    "    :param plot_error_bars: Boolean flag specifying whether to plot the error bars.\n",
    "    :param low_vary_column: String of the title of the column with points by which to create clusters.\n",
    "    :param high_vary_column: String of the title of the column with the varying points in a cluster. \n",
    "    :param error_tolerance: The tolerance range of the x values in whatever units the x_column is. \n",
    "    :param y_vary: Boolean indicating if the y_column is varying or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    # filter out unwanted charges\n",
    "    df_charge_filtered = df[(df[pv_charge] >= charge_val - (charge_val * charge_tolerance)) & (df[pv_charge] <= charge_val + (charge_val * charge_tolerance))]\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # plot correlation points\n",
    "    x = df_charge_filtered[pv_x]\n",
    "    y = df_charge_filtered[pv_y]\n",
    "    \n",
    "    if plot_error_bars:\n",
    "        # plot cluster points\n",
    "        df_groups: list[pd.DataFrame] = create_clusters(df, low_vary_column, error_tolerance)\n",
    "        means_and_stds: list[(float, float)] = get_means_and_stds(df_groups, \"SCOP:AMRF:RF01:AI_MEAS1\")\n",
    "        \n",
    "        y_cluster_points = []\n",
    "        x_cluster_points = []\n",
    "        if y_vary:  # if the points in the y_column are the varying points in a cluster\n",
    "            y_cluster_points = [cluster[0] for cluster in means_and_stds]  # average y val in each cluster\n",
    "            x_cluster_points = [float(np.mean(df_groups[df][pv_x])) for df in range(len(df_groups))]  # average x val in each cluster\n",
    "        else: \n",
    "            y_cluster_points = [float(np.mean(df_groups[df][pv_y])) for df in range(len(df_groups))]  # average y val in each cluster\n",
    "            x_cluster_points = [cluster[0] for cluster in means_and_stds]  # average x val in each cluster\n",
    "            \n",
    "        ax.scatter(x_cluster_points, y_cluster_points)\n",
    "        # create a line of best fit\n",
    "        slope, intercept = np.polyfit(x_cluster_points, y_cluster_points, deg=1)  \n",
    "        ax.axline(xy1=(0, intercept), slope=slope, label=f\"y = {slope:.3f}x + {intercept:.3f}\")\n",
    "            \n",
    "        # plot error bars\n",
    "        error = [cluster[1] for cluster in means_and_stds]\n",
    "        ax.errorbar(x_cluster_points, y_cluster_points, yerr=error, ls=\"none\")\n",
    "    else: \n",
    "        ax.scatter(x, y) \n",
    "        # create a line of best fit\n",
    "        slope, intercept = np.polyfit(x, y, deg=1)  \n",
    "        ax.axline(xy1=(0, intercept), slope=slope, label=f\"y = {slope:.3f}x + {intercept:.3f}\")\n",
    "    \n",
    "    # set labels\n",
    "    start_date = str(df_charge_filtered[\"Timestamp\"].to_list()[0])[:10]\n",
    "    end_date = str(df_charge_filtered[\"Timestamp\"].to_list()[-1])[:10]\n",
    "    ax.set_title(f\"{pv_y} vs. {pv_x} for {charge_val}pC from {start_date} to {end_date}\")\n",
    "    ax.set_xlabel(pv_x)\n",
    "    ax.set_ylabel(pv_y)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ],
   "id": "b46710a9c578ba94",
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
